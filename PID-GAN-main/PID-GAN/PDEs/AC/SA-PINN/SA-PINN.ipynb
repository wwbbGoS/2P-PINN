{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 22:44:11.854936: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               384       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,049\n",
      "Trainable params: 50,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 22:44:12.997337: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "# from plotting import newfig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n",
    "from eager_lbfgs import lbfgs, Struct\n",
    "from pyDOE import lhs\n",
    "\n",
    "\n",
    "\n",
    "#define size of the network\n",
    "layer_sizes = [2, 128, 128, 128, 128, 1]\n",
    "\n",
    "sizes_w = []\n",
    "sizes_b = []\n",
    "\n",
    "for i, width in enumerate(layer_sizes):\n",
    "    if i != 1:\n",
    "        sizes_w.append(int(width * layer_sizes[1]))\n",
    "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
    "\n",
    "\n",
    "#L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
    "def set_weights(model, w, sizes_w, sizes_b):\n",
    "        for i, layer in enumerate(model.layers[0:]):\n",
    "            start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
    "            end_weights = sum(sizes_w[:i+1]) + sum(sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(sizes_w[i] / sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "\n",
    "def get_weights(model):\n",
    "        w = []\n",
    "        for layer in model.layers[0:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "\n",
    "        w = tf.convert_to_tensor(w)\n",
    "        return w\n",
    "\n",
    "#define the neural network model\n",
    "def neural_net(layer_sizes):\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(layer_sizes[0],)))\n",
    "    for width in layer_sizes[1:-1]:\n",
    "        model.add(layers.Dense(\n",
    "            width, activation=tf.nn.tanh,\n",
    "            kernel_initializer=\"glorot_normal\"))\n",
    "    model.add(layers.Dense(\n",
    "            layer_sizes[-1], activation=None,\n",
    "            kernel_initializer=\"glorot_normal\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#define the loss\n",
    "def loss(x_f_batch, t_f_batch,\n",
    "         x0, t0, u0, x_lb,\n",
    "         t_lb, x_ub, t_ub,\n",
    "         col_weights, u_weights):\n",
    "\n",
    "    f_u_pred = f_model(x_f_batch, t_f_batch)\n",
    "    u0_pred = u_model(tf.concat([x0, t0], 1))\n",
    "\n",
    "    u_lb_pred, u_x_lb_pred, = u_x_model(u_model, x_lb, t_lb)\n",
    "    u_ub_pred, u_x_ub_pred, = u_x_model(u_model, x_ub, t_ub)\n",
    "\n",
    "    mse_0_u = tf.reduce_mean(tf.square(u_weights*(u0 - u0_pred)))\n",
    "    mse_b_u = tf.reduce_mean(tf.square(tf.math.subtract(u_lb_pred, u_ub_pred))) + \\\n",
    "              tf.reduce_mean(tf.square(tf.math.subtract(u_x_lb_pred, u_x_ub_pred)))\n",
    "\n",
    "    mse_f_u = tf.reduce_mean(tf.square(col_weights * f_u_pred[0]))\n",
    "\n",
    "    return  mse_0_u + mse_b_u + mse_f_u , tf.reduce_mean(tf.square((u0 - u0_pred))), mse_b_u, tf.reduce_mean(tf.square(f_u_pred))\n",
    "\n",
    "#define the physics-based residual, we want this to be 0\n",
    "\n",
    "@tf.function\n",
    "def f_model(x,t):\n",
    "    u = u_model(tf.concat([x, t],1))\n",
    "    u_x = tf.gradients(u, x)\n",
    "    u_xx = tf.gradients(u_x, x)\n",
    "    u_t = tf.gradients(u,t)\n",
    "    c1 = tf.constant(.0001, dtype = tf.float32)\n",
    "    c2 = tf.constant(5.0, dtype = tf.float32)\n",
    "    f_u = u_t - c1*u_xx + c2*u*u*u - c2*u\n",
    "    return f_u\n",
    "\n",
    "@tf.function\n",
    "def u_x_model(u_model, x, t):\n",
    "    u = u_model(tf.concat([x, t],1))\n",
    "    u_x = tf.gradients(u, x)\n",
    "    return u, u_x\n",
    "\n",
    "@tf.function\n",
    "def grad(model, x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        loss_value, mse_0, mse_b, mse_f = loss(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        #print(grads)\n",
    "        grads_col = tape.gradient(loss_value, col_weights)\n",
    "        grads_u = tape.gradient(loss_value, u_weights)\n",
    "        gradients_u = tape.gradient(mse_0, u_model.trainable_variables)\n",
    "        gradients_f = tape.gradient(mse_f, u_model.trainable_variables)\n",
    "\n",
    "    return loss_value, mse_0, mse_b, mse_f, grads, grads_col, grads_u, gradients_u, gradients_f\n",
    "\n",
    "\n",
    "def fit(x_f, t_f, x0, t0, u0, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights, tf_iter, newton_iter):\n",
    "\n",
    "    #Can adjust batch size for collocation points, here we set it to N_f\n",
    "    batch_sz = N_f\n",
    "    n_batches =  N_f // batch_sz\n",
    "\n",
    "    start_time = time.time()\n",
    "    #create optimizer s for the network weights, collocation point mask, and initial boundary mask\n",
    "    tf_optimizer = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\n",
    "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\n",
    "    tf_optimizer_u = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\n",
    "\n",
    "    print(\"starting Adam training\")\n",
    "\n",
    "    # For mini-batch (if used)\n",
    "    for epoch in range(tf_iter):\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            x0_batch = x0\n",
    "            t0_batch = t0\n",
    "            u0_batch = u0\n",
    "\n",
    "            x_f_batch = x_f[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "            t_f_batch = t_f[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "\n",
    "            loss_value, mse_0, mse_b, mse_f, grads, grads_col, grads_u, g_u, g_f = grad(u_model, x_f_batch, t_f_batch, x0_batch, t0_batch,  u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "\n",
    "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
    "            tf_optimizer_weights.apply_gradients(zip([-grads_col, -grads_u], [col_weights, u_weights]))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
    "            tf.print(f\"mse_0: {mse_0}  mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    #l-bfgs-b optimization\n",
    "    print(\"Starting L-BFGS training\")\n",
    "\n",
    "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      get_weights(u_model),\n",
    "      Struct(), maxIter=newton_iter, learningRate=0.8)\n",
    "\n",
    "\n",
    "#L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "def get_loss_and_flat_grad(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights):\n",
    "    def loss_and_flat_grad(w):\n",
    "        with tf.GradientTape() as tape:\n",
    "            set_weights(u_model, w, sizes_w, sizes_b)\n",
    "            loss_value, _, _, _ = loss(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        grad_flat = []\n",
    "        for g in grad:\n",
    "            grad_flat.append(tf.reshape(g, [-1]))\n",
    "        grad_flat = tf.concat(grad_flat, 0)\n",
    "        #print(loss_value, grad_flat)\n",
    "        return loss_value, grad_flat\n",
    "\n",
    "    return loss_and_flat_grad\n",
    "\n",
    "\n",
    "def predict(X_star):\n",
    "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
    "    u_star, _ = u_x_model(u_model, X_star[:,0:1],\n",
    "                     X_star[:,1:2])\n",
    "\n",
    "    f_u_star = f_model(X_star[:,0:1],\n",
    "                 X_star[:,1:2])\n",
    "\n",
    "    return u_star.numpy(), f_u_star.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and weight vectors\n",
    "\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "\n",
    "N0 = 512\n",
    "N_b = 100\n",
    "N_f = 20000\n",
    "\n",
    "col_weights = tf.Variable(tf.random.uniform([N_f, 1]))\n",
    "u_weights = tf.Variable(100*tf.random.uniform([N0, 1]))\n",
    "\n",
    "#initialize the NN\n",
    "u_model = neural_net(layer_sizes)\n",
    "\n",
    "#view the NN\n",
    "u_model.summary()\n",
    "\n",
    "\n",
    "# Import data, same data as Raissi et al\n",
    "\n",
    "data = scipy.io.loadmat('AC.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "\n",
    "\n",
    "\n",
    "#grab training points from domain\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "x0 = x[idx_x,:]\n",
    "u0 = tf.cast(Exact_u[idx_x,0:1], dtype = tf.float32)\n",
    "\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "# Grab collocation points using latin hpyercube sampling\n",
    "\n",
    "X_f = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "x_f = tf.convert_to_tensor(X_f[:,0:1], dtype=tf.float32)\n",
    "t_f = tf.convert_to_tensor(np.abs(X_f[:,1:2]), dtype=tf.float32)\n",
    "\n",
    "\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "\n",
    "x0 = tf.cast(X0[:,0:1], dtype = tf.float32)\n",
    "t0 = tf.cast(X0[:,1:2], dtype = tf.float32)\n",
    "\n",
    "x_lb = tf.convert_to_tensor(X_lb[:,0:1], dtype=tf.float32)\n",
    "t_lb = tf.convert_to_tensor(X_lb[:,1:2], dtype=tf.float32)\n",
    "\n",
    "x_ub = tf.convert_to_tensor(X_ub[:,0:1], dtype=tf.float32)\n",
    "t_ub = tf.convert_to_tensor(X_ub[:,1:2], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Adam training\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Time: 3.61\n",
      "mse_0: 0.1616050899028778  mse_b  0.2008422315120697  mse_f: 0.4143359363079071   total loss: 559.1511840820312\n",
      "It: 100, Time: 70.86\n",
      "mse_0: 0.10267822444438934  mse_b  0.0033408990129828453  mse_f: 0.9199413061141968   total loss: 369.904052734375\n",
      "It: 200, Time: 71.90\n",
      "mse_0: 0.10209926217794418  mse_b  0.000933764735236764  mse_f: 0.4933348596096039   total loss: 369.6451721191406\n",
      "It: 300, Time: 69.79\n",
      "mse_0: 0.07052236050367355  mse_b  0.050610266625881195  mse_f: 0.6819208860397339   total loss: 259.1938171386719\n",
      "It: 400, Time: 68.81\n",
      "mse_0: 0.016340401023626328  mse_b  14.844330787658691  mse_f: 1.2956078052520752   total loss: 69.20645904541016\n",
      "It: 500, Time: 70.53\n",
      "mse_0: 0.012946998700499535  mse_b  4.727746963500977  mse_f: 2.3401293754577637   total loss: 50.498443603515625\n",
      "It: 600, Time: 69.91\n",
      "mse_0: 0.010809150524437428  mse_b  3.5725479125976562  mse_f: 1.4338330030441284   total loss: 42.758968353271484\n",
      "It: 700, Time: 70.82\n",
      "mse_0: 0.009532410651445389  mse_b  2.294241189956665  mse_f: 0.684964656829834   total loss: 36.62017822265625\n",
      "It: 800, Time: 70.62\n",
      "mse_0: 0.006855863146483898  mse_b  2.286863088607788  mse_f: 0.7774537205696106   total loss: 28.14145278930664\n",
      "It: 900, Time: 68.71\n",
      "mse_0: 0.0011117151007056236  mse_b  5.660609722137451  mse_f: 1.2609587907791138   total loss: 12.107805252075195\n",
      "It: 1000, Time: 69.64\n",
      "mse_0: 0.0002550921053625643  mse_b  3.6010653972625732  mse_f: 1.2249517440795898   total loss: 7.521099090576172\n",
      "It: 1100, Time: 69.99\n",
      "mse_0: 0.00027438573306426406  mse_b  2.9042725563049316  mse_f: 1.2066665887832642   total loss: 7.383774280548096\n",
      "It: 1200, Time: 71.36\n",
      "mse_0: 0.00011205766350030899  mse_b  2.8455255031585693  mse_f: 1.0888274908065796   total loss: 7.032366752624512\n",
      "It: 1300, Time: 70.38\n",
      "mse_0: 6.319407111732289e-05  mse_b  2.9386162757873535  mse_f: 0.9566326141357422   total loss: 6.877194404602051\n",
      "It: 1400, Time: 74.76\n",
      "mse_0: 9.120184404309839e-05  mse_b  2.836399555206299  mse_f: 0.7841587066650391   total loss: 6.520727157592773\n",
      "It: 1500, Time: 73.53\n",
      "mse_0: 6.516416760860011e-05  mse_b  2.638162612915039  mse_f: 0.6759098768234253   total loss: 6.032912731170654\n",
      "It: 1600, Time: 71.68\n",
      "mse_0: 7.873473077779636e-05  mse_b  2.1054890155792236  mse_f: 0.5215514302253723   total loss: 5.10054874420166\n",
      "It: 1700, Time: 71.38\n",
      "mse_0: 6.06326138949953e-05  mse_b  1.3592016696929932  mse_f: 0.34585878252983093   total loss: 3.291243076324463\n",
      "It: 1800, Time: 70.63\n",
      "mse_0: 7.635828660568222e-05  mse_b  0.7720429301261902  mse_f: 0.29559558629989624   total loss: 1.968651294708252\n",
      "It: 1900, Time: 67.25\n",
      "mse_0: 4.4953194446861744e-05  mse_b  0.3967115581035614  mse_f: 0.1608295440673828   total loss: 1.0883606672286987\n",
      "It: 2000, Time: 72.20\n",
      "mse_0: 4.791799437953159e-05  mse_b  0.22496214509010315  mse_f: 0.13854393362998962   total loss: 0.8933219909667969\n",
      "It: 2100, Time: 71.72\n",
      "mse_0: 3.585261947591789e-05  mse_b  0.17106325924396515  mse_f: 0.12454721331596375   total loss: 0.7622752785682678\n",
      "It: 2200, Time: 73.61\n",
      "mse_0: 3.80200071958825e-05  mse_b  0.14326418936252594  mse_f: 0.10424470156431198   total loss: 0.6924492120742798\n",
      "It: 2300, Time: 69.45\n",
      "mse_0: 3.791692142840475e-05  mse_b  0.12530063092708588  mse_f: 0.08894676715135574   total loss: 0.634235143661499\n",
      "It: 2400, Time: 69.62\n",
      "mse_0: 3.4073495044140145e-05  mse_b  0.12008190900087357  mse_f: 0.07733345031738281   total loss: 0.5880640745162964\n",
      "It: 2500, Time: 69.56\n",
      "mse_0: 3.429076969041489e-05  mse_b  0.11225354671478271  mse_f: 0.06868281215429306   total loss: 0.5491185784339905\n",
      "It: 2600, Time: 72.66\n",
      "mse_0: 3.423251473577693e-05  mse_b  0.10831507295370102  mse_f: 0.060801293700933456   total loss: 0.522196888923645\n",
      "It: 2700, Time: 68.92\n",
      "mse_0: 3.358862886670977e-05  mse_b  0.09791793674230576  mse_f: 0.054997868835926056   total loss: 0.48792004585266113\n",
      "It: 2800, Time: 70.68\n",
      "mse_0: 3.3685377275105566e-05  mse_b  0.09006952494382858  mse_f: 0.050911229103803635   total loss: 0.46365082263946533\n",
      "It: 2900, Time: 73.37\n",
      "mse_0: 3.195535464328714e-05  mse_b  0.08316154778003693  mse_f: 0.04722052812576294   total loss: 0.43809014558792114\n",
      "It: 3000, Time: 69.36\n",
      "mse_0: 3.131750418106094e-05  mse_b  0.07743629813194275  mse_f: 0.04330442100763321   total loss: 0.4141676127910614\n",
      "It: 3100, Time: 67.92\n",
      "mse_0: 2.9700131563004106e-05  mse_b  0.07046383619308472  mse_f: 0.03976217657327652   total loss: 0.3862651586532593\n",
      "It: 3200, Time: 69.82\n",
      "mse_0: 2.736908572842367e-05  mse_b  0.06414499878883362  mse_f: 0.03642001003026962   total loss: 0.357550710439682\n",
      "It: 3300, Time: 70.94\n",
      "mse_0: 2.714977745199576e-05  mse_b  0.0585402175784111  mse_f: 0.03416847065091133   total loss: 0.3449411988258362\n",
      "It: 3400, Time: 69.78\n",
      "mse_0: 2.374395808146801e-05  mse_b  0.05332755297422409  mse_f: 0.03141142427921295   total loss: 0.3183271586894989\n",
      "It: 3500, Time: 73.72\n",
      "mse_0: 2.3099635654943995e-05  mse_b  0.048241615295410156  mse_f: 0.02895236201584339   total loss: 0.30265164375305176\n",
      "It: 3600, Time: 70.15\n",
      "mse_0: 2.350412069063168e-05  mse_b  0.043549858033657074  mse_f: 0.02842673659324646   total loss: 0.3017641007900238\n",
      "It: 3700, Time: 70.38\n",
      "mse_0: 1.868236722657457e-05  mse_b  0.038502246141433716  mse_f: 0.026064995676279068   total loss: 0.27011537551879883\n",
      "It: 3800, Time: 71.94\n",
      "mse_0: 1.7904938431456685e-05  mse_b  0.03409197926521301  mse_f: 0.02491123415529728   total loss: 0.2603025734424591\n",
      "It: 3900, Time: 70.31\n",
      "mse_0: 1.8565191567176953e-05  mse_b  0.0305262953042984  mse_f: 0.025277871638536453   total loss: 0.26523444056510925\n",
      "It: 4000, Time: 70.85\n",
      "mse_0: 1.526917731098365e-05  mse_b  0.02700178697705269  mse_f: 0.023081371560692787   total loss: 0.24069586396217346\n",
      "It: 4100, Time: 68.79\n",
      "mse_0: 1.3740143913310021e-05  mse_b  0.024177908897399902  mse_f: 0.02225111983716488   total loss: 0.2309674173593521\n",
      "It: 4200, Time: 69.25\n",
      "mse_0: 1.4178925994201563e-05  mse_b  0.021817149594426155  mse_f: 0.022340985015034676   total loss: 0.23413267731666565\n",
      "It: 4300, Time: 69.02\n",
      "mse_0: 1.7999262126977555e-05  mse_b  0.019212650135159492  mse_f: 0.019992822781205177   total loss: 0.23261894285678864\n",
      "It: 4400, Time: 70.97\n",
      "mse_0: 1.42913922900334e-05  mse_b  0.01725219003856182  mse_f: 0.02071140520274639   total loss: 0.22594544291496277\n",
      "It: 4500, Time: 71.55\n",
      "mse_0: 1.60294548550155e-05  mse_b  0.014793495647609234  mse_f: 0.01827145181596279   total loss: 0.215340718626976\n",
      "It: 4600, Time: 71.02\n",
      "mse_0: 1.1548560905794147e-05  mse_b  0.012906997464597225  mse_f: 0.018854176625609398   total loss: 0.20515529811382294\n",
      "It: 4700, Time: 70.00\n",
      "mse_0: 1.2833890650654212e-05  mse_b  0.010654366575181484  mse_f: 0.016791323199868202   total loss: 0.19447636604309082\n",
      "It: 4800, Time: 70.22\n",
      "mse_0: 8.259194146376103e-06  mse_b  0.008992482908070087  mse_f: 0.017114028334617615   total loss: 0.1817682534456253\n",
      "It: 4900, Time: 75.39\n",
      "mse_0: 8.479153621010482e-06  mse_b  0.007359749637544155  mse_f: 0.015729239210486412   total loss: 0.17191049456596375\n",
      "It: 5000, Time: 73.77\n",
      "mse_0: 6.276190561038675e-06  mse_b  0.006295876111835241  mse_f: 0.015829959884285927   total loss: 0.1655392050743103\n",
      "It: 5100, Time: 70.83\n",
      "mse_0: 7.126644050003961e-06  mse_b  0.005330564454197884  mse_f: 0.014745119959115982   total loss: 0.16011780500411987\n",
      "It: 5200, Time: 70.81\n",
      "mse_0: 5.5297605285886675e-06  mse_b  0.004745537880808115  mse_f: 0.015033978037536144   total loss: 0.1576869785785675\n",
      "It: 5300, Time: 67.95\n",
      "mse_0: 9.26836219150573e-06  mse_b  0.004075337667018175  mse_f: 0.013704479672014713   total loss: 0.1603783369064331\n",
      "It: 5400, Time: 71.18\n",
      "mse_0: 7.3989185693790205e-06  mse_b  0.003854144364595413  mse_f: 0.014697983860969543   total loss: 0.16383321583271027\n",
      "It: 5500, Time: 68.11\n",
      "mse_0: 9.554583812132478e-06  mse_b  0.0032345072831958532  mse_f: 0.012952916324138641   total loss: 0.15751999616622925\n",
      "It: 5600, Time: 68.13\n",
      "mse_0: 3.87392719858326e-06  mse_b  0.003023602534085512  mse_f: 0.01345047727227211   total loss: 0.14415210485458374\n",
      "It: 5700, Time: 71.35\n",
      "mse_0: 4.093546522199176e-06  mse_b  0.0027481000870466232  mse_f: 0.01330508105456829   total loss: 0.1454341560602188\n",
      "It: 5800, Time: 72.51\n",
      "mse_0: 1.0416270924906712e-05  mse_b  0.0023334340658038855  mse_f: 0.011972606182098389   total loss: 0.15661747753620148\n",
      "It: 5900, Time: 68.64\n",
      "mse_0: 4.2193123590550385e-06  mse_b  0.0022817812860012054  mse_f: 0.012828800827264786   total loss: 0.14570321142673492\n",
      "It: 6000, Time: 69.02\n",
      "mse_0: 3.203157120879041e-06  mse_b  0.0020390460267663  mse_f: 0.012167515233159065   total loss: 0.13816706836223602\n",
      "It: 6100, Time: 69.43\n",
      "mse_0: 9.421190952707548e-06  mse_b  0.001754813827574253  mse_f: 0.01118683535605669   total loss: 0.15196701884269714\n",
      "It: 6200, Time: 71.23\n",
      "mse_0: 4.470284693525173e-06  mse_b  0.001769708702340722  mse_f: 0.012117018923163414   total loss: 0.14729183912277222\n",
      "It: 6300, Time: 70.19\n",
      "mse_0: 3.1318045330408495e-06  mse_b  0.001557916752062738  mse_f: 0.011287301778793335   total loss: 0.1367311328649521\n",
      "It: 6400, Time: 70.53\n",
      "mse_0: 8.548109690309502e-06  mse_b  0.0013452835846692324  mse_f: 0.010488430969417095   total loss: 0.14924356341362\n",
      "It: 6500, Time: 71.58\n",
      "mse_0: 4.801671821041964e-06  mse_b  0.0013899486511945724  mse_f: 0.011457200162112713   total loss: 0.15055617690086365\n",
      "It: 6600, Time: 69.51\n",
      "mse_0: 3.830779860436451e-06  mse_b  0.001152889453805983  mse_f: 0.010405994020402431   total loss: 0.13831639289855957\n",
      "It: 6700, Time: 71.21\n",
      "mse_0: 6.90013803250622e-06  mse_b  0.0010138595243915915  mse_f: 0.009931434877216816   total loss: 0.14600305259227753\n",
      "It: 6800, Time: 67.63\n",
      "mse_0: 4.928428097628057e-06  mse_b  0.0010857691522687674  mse_f: 0.010887140408158302   total loss: 0.1540783792734146\n",
      "It: 6900, Time: 69.83\n",
      "mse_0: 3.516138576742378e-06  mse_b  0.0008774733869358897  mse_f: 0.00986629631370306   total loss: 0.14036092162132263\n",
      "It: 7000, Time: 70.73\n",
      "mse_0: 8.858865840011276e-06  mse_b  0.0007469508564099669  mse_f: 0.009274349547922611   total loss: 0.1539737433195114\n",
      "It: 7100, Time: 69.75\n",
      "mse_0: 3.7265658647811506e-06  mse_b  0.0008234366541728377  mse_f: 0.010128284804522991   total loss: 0.151619091629982\n",
      "It: 7200, Time: 69.58\n",
      "mse_0: 2.8539989216369577e-06  mse_b  0.0007181315449997783  mse_f: 0.009679896757006645   total loss: 0.1464800238609314\n",
      "It: 7300, Time: 71.54\n",
      "mse_0: 9.896371921058744e-06  mse_b  0.0005658298032358289  mse_f: 0.008766140788793564   total loss: 0.16079044342041016\n",
      "It: 7400, Time: 73.02\n",
      "mse_0: 3.085160642513074e-06  mse_b  0.0005821131053380668  mse_f: 0.009191536344587803   total loss: 0.14799076318740845\n",
      "It: 7500, Time: 69.12\n",
      "mse_0: 4.574942977342289e-06  mse_b  0.0006338515086099505  mse_f: 0.009658570401370525   total loss: 0.16375894844532013\n",
      "It: 7600, Time: 70.46\n",
      "mse_0: 5.7095189731626306e-06  mse_b  0.00044977490324527025  mse_f: 0.008597035892307758   total loss: 0.15557080507278442\n",
      "It: 7700, Time: 68.76\n",
      "mse_0: 8.38383857626468e-06  mse_b  0.0003989748365711421  mse_f: 0.008323471993207932   total loss: 0.1640544831752777\n",
      "It: 7800, Time: 71.10\n",
      "mse_0: 3.529081368469633e-06  mse_b  0.0004741921729873866  mse_f: 0.009053261019289494   total loss: 0.16385135054588318\n",
      "It: 7900, Time: 72.61\n",
      "mse_0: 3.3889923543029e-06  mse_b  0.0004278269479982555  mse_f: 0.008869176730513573   total loss: 0.16485348343849182\n",
      "It: 8000, Time: 74.01\n",
      "mse_0: 9.677190973889083e-06  mse_b  0.0003035268746316433  mse_f: 0.007944419980049133   total loss: 0.17430511116981506\n",
      "It: 8100, Time: 70.19\n",
      "mse_0: 5.115928615850862e-06  mse_b  0.00028972423751838505  mse_f: 0.008124659769237041   total loss: 0.16685639321804047\n",
      "It: 8200, Time: 70.54\n",
      "mse_0: 4.771706699102651e-06  mse_b  0.00041002518264576793  mse_f: 0.008803385309875011   total loss: 0.18256455659866333\n",
      "It: 8300, Time: 69.90\n",
      "mse_0: 4.435305527294986e-06  mse_b  0.0002541267895139754  mse_f: 0.00803331844508648   total loss: 0.17229975759983063\n",
      "It: 8400, Time: 70.88\n",
      "mse_0: 1.113919643103145e-05  mse_b  0.00022954845917411149  mse_f: 0.007509069051593542   total loss: 0.18888579308986664\n",
      "It: 8500, Time: 72.06\n",
      "mse_0: 3.891708729497623e-06  mse_b  0.00025451285182498395  mse_f: 0.008069195784628391   total loss: 0.1811710000038147\n",
      "It: 8600, Time: 69.84\n",
      "mse_0: 4.579972028295742e-06  mse_b  0.00031295110238716006  mse_f: 0.008276672102510929   total loss: 0.19284379482269287\n",
      "It: 8700, Time: 70.30\n",
      "mse_0: 8.884410817699973e-06  mse_b  0.00023127342865336686  mse_f: 0.00734851835295558   total loss: 0.1929921656847\n",
      "It: 8800, Time: 69.92\n",
      "mse_0: 1.1954420187976211e-05  mse_b  0.00020940981630701572  mse_f: 0.007165565621107817   total loss: 0.20363427698612213\n",
      "It: 8900, Time: 71.19\n",
      "mse_0: 1.1706025361490902e-05  mse_b  0.0001750460360199213  mse_f: 0.007126977667212486   total loss: 0.2068357616662979\n",
      "It: 9000, Time: 69.51\n",
      "mse_0: 4.972019269189332e-06  mse_b  0.0002852331381291151  mse_f: 0.007568879518657923   total loss: 0.20084810256958008\n",
      "It: 9100, Time: 72.27\n",
      "mse_0: 5.887823135708459e-06  mse_b  0.00021494334214366972  mse_f: 0.007913123816251755   total loss: 0.21764488518238068\n",
      "It: 9200, Time: 71.73\n",
      "mse_0: 6.602521352760959e-06  mse_b  0.00014611828373745084  mse_f: 0.007233685348182917   total loss: 0.2096957266330719\n",
      "It: 9300, Time: 69.14\n",
      "mse_0: 1.3669943655258976e-05  mse_b  0.00027371649048291147  mse_f: 0.006791223771870136   total loss: 0.22684842348098755\n",
      "It: 9400, Time: 68.43\n",
      "mse_0: 5.600830263574608e-06  mse_b  0.0002661019389051944  mse_f: 0.007394731044769287   total loss: 0.22288252413272858\n",
      "It: 9500, Time: 70.36\n",
      "mse_0: 5.8951136452378705e-06  mse_b  0.0003533790586516261  mse_f: 0.007466066628694534   total loss: 0.23204518854618073\n",
      "It: 9600, Time: 72.58\n",
      "mse_0: 1.2735079508274794e-05  mse_b  0.0002662204497028142  mse_f: 0.006648344453424215   total loss: 0.23807743191719055\n",
      "It: 9700, Time: 66.55\n",
      "mse_0: 1.1176076441188343e-05  mse_b  0.00016780942678451538  mse_f: 0.006666043773293495   total loss: 0.2399309277534485\n",
      "It: 9800, Time: 68.07\n",
      "mse_0: 6.960426617297344e-06  mse_b  0.0003080190799664706  mse_f: 0.007292829919606447   total loss: 0.2513040602207184\n",
      "It: 9900, Time: 69.80\n",
      "mse_0: 7.1136087171908e-06  mse_b  0.0003004816244356334  mse_f: 0.007059140130877495   total loss: 0.25217849016189575\n",
      "Starting L-BFGS training\n",
      "Step  10 loss 0.25405 \n",
      "Step  20 loss 0.25404 \n",
      "Step  30 loss 0.25403 \n",
      "Step  40 loss 0.25397 \n",
      "Step  50 loss 0.25381 \n",
      "Step  60 loss 0.25365 \n",
      "Step  70 loss 0.25357 \n",
      "Step  80 loss 0.25350 \n",
      "Step  90 loss 0.25332 \n",
      "Step 100 loss 0.25313 \n",
      "Step 110 loss 0.25290 \n",
      "Step 120 loss 0.25274 \n",
      "Step 130 loss 0.25255 \n",
      "Step 140 loss 0.25247 \n",
      "Step 150 loss 0.25230 \n",
      "Step 160 loss 0.25199 \n",
      "Step 170 loss 0.25180 \n",
      "Step 180 loss 0.25168 \n",
      "Step 190 loss 0.25150 \n",
      "Step 200 loss 0.25125 \n",
      "Step 210 loss 0.25103 \n",
      "Step 220 loss 0.25091 \n",
      "Step 230 loss 0.25084 \n",
      "Step 240 loss 0.25077 \n",
      "Step 250 loss 0.25071 \n",
      "Step 260 loss 0.25062 \n",
      "Step 270 loss 0.25034 \n",
      "Step 280 loss 0.25028 \n",
      "Step 290 loss 0.25021 \n",
      "Step 300 loss 0.25012 \n",
      "Step 310 loss 0.25007 \n",
      "Step 320 loss 0.24996 \n",
      "Step 330 loss 0.24990 \n",
      "Step 340 loss 0.24976 \n",
      "Step 350 loss 0.24956 \n",
      "Step 360 loss 0.24945 \n",
      "Step 370 loss 0.24929 \n",
      "Step 380 loss 0.24918 \n",
      "Step 390 loss 0.24893 \n",
      "Step 400 loss 0.24886 \n",
      "Step 410 loss 0.24875 \n",
      "Step 420 loss 0.24854 \n",
      "Step 430 loss 0.24845 \n",
      "Step 440 loss 0.24833 \n",
      "Step 450 loss 0.24823 \n",
      "Step 460 loss 0.24820 \n",
      "Step 470 loss 0.24813 \n",
      "Step 480 loss 0.24796 \n",
      "Step 490 loss 0.24791 \n",
      "Step 500 loss 0.24783 \n",
      "Step 510 loss 0.24773 \n",
      "Step 520 loss 0.24769 \n",
      "Step 530 loss 0.24757 \n",
      "Step 540 loss 0.24747 \n",
      "Step 550 loss 0.24735 \n",
      "Step 560 loss 0.24726 \n",
      "Step 570 loss 0.24721 \n",
      "Step 580 loss 0.24717 \n",
      "Step 590 loss 0.24710 \n",
      "Step 600 loss 0.24702 \n",
      "Step 610 loss 0.24694 \n",
      "Step 620 loss 0.24687 \n",
      "Step 630 loss 0.24682 \n",
      "Step 640 loss 0.24678 \n",
      "Step 650 loss 0.24674 \n",
      "Step 660 loss 0.24669 \n",
      "Step 670 loss 0.24658 \n",
      "Step 680 loss 0.24652 \n",
      "Step 690 loss 0.24648 \n",
      "Step 700 loss 0.24641 \n",
      "Step 710 loss 0.24633 \n",
      "Step 720 loss 0.24622 \n",
      "Step 730 loss 0.24612 \n",
      "Step 740 loss 0.24612 \n",
      "Step 750 loss 0.24600 \n",
      "Step 760 loss 0.24587 \n",
      "Step 770 loss 0.24571 \n",
      "Step 780 loss 0.24554 \n",
      "Step 790 loss 0.24544 \n",
      "Step 800 loss 0.24526 \n",
      "Step 810 loss 0.24516 \n",
      "Step 820 loss 0.24498 \n",
      "Step 830 loss 0.24482 \n",
      "Step 840 loss 0.24467 \n",
      "Step 850 loss 0.24460 \n",
      "Step 860 loss 0.24450 \n",
      "Step 870 loss 0.24439 \n",
      "Step 880 loss 0.24447 \n",
      "Step 890 loss 0.24416 \n",
      "Step 900 loss 0.24399 \n",
      "Step 910 loss 0.24395 \n",
      "Step 920 loss 0.24387 \n",
      "Step 930 loss 0.24380 \n",
      "Step 940 loss 0.24372 \n",
      "Step 950 loss 0.24357 \n",
      "Step 960 loss 0.24347 \n",
      "Step 970 loss 0.24326 \n",
      "Step 980 loss 0.24658 \n",
      "Step 990 loss 0.24305 \n",
      "Step 1000 loss 0.24300 \n",
      "Step 1010 loss 0.24294 \n",
      "Step 1020 loss 0.24285 \n",
      "Step 1030 loss 0.24271 \n",
      "Step 1040 loss 0.24258 \n",
      "Step 1050 loss 0.24242 \n",
      "Step 1060 loss 0.24225 \n",
      "Step 1070 loss 0.24197 \n",
      "Step 1080 loss 0.24180 \n",
      "Step 1090 loss 0.24168 \n",
      "Step 1100 loss 0.24149 \n",
      "Step 1110 loss 0.24143 \n",
      "Step 1120 loss 0.24128 \n",
      "Step 1130 loss 0.24116 \n",
      "Step 1140 loss 0.24101 \n",
      "Step 1150 loss 0.24088 \n",
      "Step 1160 loss 0.24070 \n",
      "Step 1170 loss 0.24059 \n",
      "Step 1180 loss 0.24050 \n",
      "Step 1190 loss 0.24037 \n",
      "Step 1200 loss 0.24010 \n",
      "Step 1210 loss 0.23999 \n",
      "Step 1220 loss 0.23971 \n",
      "Step 1230 loss 0.23942 \n",
      "Step 1240 loss 0.23910 \n",
      "Step 1250 loss 0.23900 \n",
      "Step 1260 loss 0.23871 \n",
      "Step 1270 loss 0.23861 \n",
      "Step 1280 loss 0.23851 \n",
      "Step 1290 loss 0.23839 \n",
      "Step 1300 loss 0.23960 \n",
      "Step 1310 loss 0.23817 \n",
      "Step 1320 loss 0.23810 \n",
      "Step 1330 loss 0.23804 \n",
      "Step 1340 loss 0.23794 \n",
      "Step 1350 loss 0.23789 \n",
      "Step 1360 loss 0.23784 \n",
      "Step 1370 loss 0.23775 \n",
      "Step 1380 loss 0.23765 \n",
      "Step 1390 loss 0.23751 \n",
      "Step 1400 loss 0.23740 \n",
      "Step 1410 loss 0.23753 \n",
      "Step 1420 loss 0.23726 \n",
      "Step 1430 loss 0.23713 \n",
      "Step 1440 loss 0.23694 \n",
      "Step 1450 loss 0.23680 \n",
      "Step 1460 loss 0.23667 \n",
      "Step 1470 loss 0.23645 \n",
      "Step 1480 loss 0.23631 \n",
      "Step 1490 loss 0.23605 \n",
      "Step 1500 loss 0.23587 \n",
      "Step 1510 loss 0.23569 \n",
      "Step 1520 loss 0.23553 \n",
      "Step 1530 loss 0.23513 \n",
      "Step 1540 loss 0.23483 \n",
      "Step 1550 loss 0.23462 \n",
      "Step 1560 loss 0.23447 \n",
      "Step 1570 loss 0.23428 \n",
      "Step 1580 loss 0.23408 \n",
      "Step 1590 loss 0.23413 \n",
      "Step 1600 loss 0.23356 \n",
      "Step 1610 loss 0.23338 \n",
      "Step 1620 loss 0.23319 \n",
      "Step 1630 loss 0.23286 \n",
      "Step 1640 loss 0.23255 \n",
      "Step 1650 loss 0.23212 \n",
      "Step 1660 loss 0.23155 \n",
      "Step 1670 loss 0.23062 \n",
      "Step 1680 loss 0.22953 \n",
      "Step 1690 loss 0.22849 \n",
      "Step 1700 loss 0.22700 \n",
      "Step 1710 loss 0.22565 \n",
      "Step 1720 loss 0.22349 \n",
      "Step 1730 loss 0.22274 \n",
      "Step 1740 loss 0.22205 \n",
      "Step 1750 loss 0.22110 \n",
      "Step 1760 loss 0.22055 \n",
      "Step 1770 loss 0.21974 \n",
      "Step 1780 loss 0.21894 \n",
      "Step 1790 loss 0.21855 \n",
      "Step 1800 loss 0.21818 \n",
      "Step 1810 loss 0.21788 \n",
      "Step 1820 loss 0.21749 \n",
      "Step 1830 loss 0.21702 \n",
      "Step 1840 loss 0.21640 \n",
      "Step 1850 loss 0.21612 \n",
      "Step 1860 loss 0.21603 \n",
      "Step 1870 loss 0.21579 \n",
      "Step 1880 loss 0.21553 \n",
      "Step 1890 loss 0.21528 \n",
      "Step 1900 loss 0.21509 \n",
      "Step 1910 loss 0.21495 \n",
      "Step 1920 loss 0.21485 \n",
      "Step 1930 loss 0.21475 \n",
      "Step 1940 loss 0.21463 \n",
      "Step 1950 loss 0.21471 \n",
      "Step 1960 loss 0.21432 \n",
      "Step 1970 loss 0.21416 \n",
      "Step 1980 loss 0.21402 \n",
      "Step 1990 loss 0.21391 \n",
      "Step 2000 loss 0.21374 \n",
      "Step 2010 loss 0.21360 \n",
      "Step 2020 loss 0.21349 \n",
      "Step 2030 loss 0.21335 \n",
      "Step 2040 loss 0.21311 \n",
      "Step 2050 loss 0.21301 \n",
      "Step 2060 loss 0.21288 \n",
      "Step 2070 loss 0.21278 \n",
      "Step 2080 loss 0.21264 \n",
      "Step 2090 loss 0.21249 \n",
      "Step 2100 loss 0.21240 \n",
      "Step 2110 loss 0.21211 \n",
      "Step 2120 loss 0.21204 \n",
      "Step 2130 loss 0.21192 \n",
      "Step 2140 loss 0.21178 \n",
      "Step 2150 loss 0.21166 \n",
      "Step 2160 loss 0.21156 \n",
      "Step 2170 loss 0.21129 \n",
      "Step 2180 loss 0.21113 \n",
      "Step 2190 loss 0.21101 \n",
      "Step 2200 loss 0.21077 \n",
      "Step 2210 loss 0.21039 \n",
      "Step 2220 loss 0.21022 \n",
      "Step 2230 loss 0.20988 \n",
      "Step 2240 loss 0.20962 \n",
      "Step 2250 loss 0.20931 \n",
      "Step 2260 loss 0.20913 \n",
      "Step 2270 loss 0.20897 \n",
      "Step 2280 loss 0.20875 \n",
      "Step 2290 loss 0.20849 \n",
      "Step 2300 loss 0.20821 \n",
      "Step 2310 loss 0.20792 \n",
      "Step 2320 loss 0.20765 \n",
      "Step 2330 loss 0.20753 \n",
      "Step 2340 loss 0.20716 \n",
      "Step 2350 loss 0.20686 \n",
      "Step 2360 loss 0.20668 \n",
      "Step 2370 loss 0.20653 \n",
      "Step 2380 loss 0.20630 \n",
      "Step 2390 loss 0.20603 \n",
      "Step 2400 loss 0.20582 \n",
      "Step 2410 loss 0.20560 \n",
      "Step 2420 loss 0.20541 \n",
      "Step 2430 loss 0.20522 \n",
      "Step 2440 loss 0.20489 \n",
      "Step 2450 loss 0.20466 \n",
      "Step 2460 loss 0.20446 \n",
      "Step 2470 loss 0.20425 \n",
      "Step 2480 loss 0.20413 \n",
      "Step 2490 loss 0.20386 \n",
      "Step 2500 loss 0.20368 \n",
      "Step 2510 loss 0.20350 \n",
      "Step 2520 loss 0.20337 \n",
      "Step 2530 loss 0.20326 \n",
      "Step 2540 loss 0.20295 \n",
      "Step 2550 loss 0.20263 \n",
      "Step 2560 loss 0.20241 \n",
      "Step 2570 loss 0.20231 \n",
      "Step 2580 loss 0.20211 \n",
      "Step 2590 loss 0.20192 \n",
      "Step 2600 loss 0.20176 \n",
      "Step 2610 loss 0.20154 \n",
      "Step 2620 loss 0.20109 \n",
      "Step 2630 loss 0.20064 \n",
      "Step 2640 loss 0.20031 \n",
      "Step 2650 loss 0.19991 \n",
      "Step 2660 loss 0.19950 \n",
      "Step 2670 loss 0.19890 \n",
      "Step 2680 loss 0.19854 \n",
      "Step 2690 loss 0.19798 \n",
      "Step 2700 loss 0.19788 \n",
      "Step 2710 loss 0.19721 \n",
      "Step 2720 loss 0.19647 \n",
      "Step 2730 loss 0.19562 \n",
      "Step 2740 loss 0.19491 \n",
      "Step 2750 loss 0.19362 \n",
      "Step 2760 loss 0.19305 \n",
      "Step 2770 loss 0.19271 \n",
      "Step 2780 loss 0.19206 \n",
      "Step 2790 loss 0.19102 \n",
      "Step 2800 loss 0.18921 \n",
      "Step 2810 loss 0.18722 \n",
      "Step 2820 loss 0.18513 \n",
      "Step 2830 loss 0.18216 \n",
      "Step 2840 loss 0.17878 \n",
      "Step 2850 loss 0.17605 \n",
      "Step 2860 loss 0.17191 \n",
      "Step 2870 loss 0.16925 \n",
      "Step 2880 loss 0.16641 \n",
      "Step 2890 loss 0.16489 \n",
      "Step 2900 loss 0.16058 \n",
      "Step 2910 loss 0.15771 \n",
      "Step 2920 loss 0.15542 \n",
      "Step 2930 loss 0.15276 \n",
      "Step 2940 loss 0.15026 \n",
      "Step 2950 loss 0.14817 \n",
      "Step 2960 loss 0.14651 \n",
      "Step 2970 loss 0.14414 \n",
      "Step 2980 loss 0.14299 \n",
      "Step 2990 loss 0.14174 \n",
      "Step 3000 loss 0.14032 \n",
      "Step 3010 loss 0.13863 \n",
      "Step 3020 loss 0.13723 \n",
      "Step 3030 loss 0.13639 \n",
      "Step 3040 loss 0.13519 \n",
      "Step 3050 loss 0.13394 \n",
      "Step 3060 loss 0.13272 \n",
      "Step 3070 loss 0.13150 \n",
      "Step 3080 loss 0.13069 \n",
      "Step 3090 loss 0.12891 \n",
      "Step 3100 loss 0.12814 \n",
      "Step 3110 loss 0.12778 \n",
      "Step 3120 loss 0.12698 \n",
      "Step 3130 loss 0.12586 \n",
      "Step 3140 loss 0.12498 \n",
      "Step 3150 loss 0.12433 \n",
      "Step 3160 loss 0.12416 \n",
      "Step 3170 loss 0.12336 \n",
      "Step 3180 loss 0.12281 \n",
      "Step 3190 loss 0.12230 \n",
      "Step 3200 loss 0.12211 \n",
      "Step 3210 loss 0.12151 \n",
      "Step 3220 loss 0.12060 \n",
      "Step 3230 loss 0.11988 \n",
      "Step 3240 loss 0.12074 \n",
      "Step 3250 loss 0.11891 \n",
      "Step 3260 loss 0.11855 \n",
      "Step 3270 loss 0.11815 \n",
      "Step 3280 loss 0.11773 \n",
      "Step 3290 loss 0.11775 \n",
      "Step 3300 loss 0.11724 \n",
      "Step 3310 loss 0.11696 \n",
      "Step 3320 loss 0.11663 \n",
      "Step 3330 loss 0.11628 \n",
      "Step 3340 loss 0.11594 \n",
      "Step 3350 loss 0.11546 \n",
      "Step 3360 loss 0.11487 \n",
      "Step 3370 loss 0.11434 \n",
      "Step 3380 loss 0.11413 \n",
      "Step 3390 loss 0.11339 \n",
      "Step 3400 loss 0.11312 \n",
      "Step 3410 loss 0.11272 \n",
      "Step 3420 loss 0.11216 \n",
      "Step 3430 loss 0.11183 \n",
      "Step 3440 loss 0.11130 \n",
      "Step 3450 loss 0.11056 \n",
      "Step 3460 loss 0.11018 \n",
      "Step 3470 loss 0.10952 \n",
      "Step 3480 loss 0.10877 \n",
      "Step 3490 loss 0.10758 \n",
      "Step 3500 loss 0.10666 \n",
      "Step 3510 loss 0.10611 \n",
      "Step 3520 loss 0.10547 \n",
      "Step 3530 loss 0.10490 \n",
      "Step 3540 loss 0.10401 \n",
      "Step 3550 loss 0.10327 \n",
      "Step 3560 loss 0.10229 \n",
      "Step 3570 loss 0.10152 \n",
      "Step 3580 loss 0.10019 \n",
      "Step 3590 loss 0.09888 \n",
      "Step 3600 loss 0.09806 \n",
      "Step 3610 loss 0.09710 \n",
      "Step 3620 loss 0.09650 \n",
      "Step 3630 loss 0.09555 \n",
      "Step 3640 loss 0.09337 \n",
      "Step 3650 loss 0.09222 \n",
      "Step 3660 loss 0.09156 \n",
      "Step 3670 loss 0.08981 \n",
      "Step 3680 loss 0.08834 \n",
      "Step 3690 loss 0.08713 \n",
      "Step 3700 loss 0.08530 \n",
      "Step 3710 loss 0.08430 \n",
      "Step 3720 loss 0.08310 \n",
      "Step 3730 loss 0.08187 \n",
      "Step 3740 loss 0.08020 \n",
      "Step 3750 loss 0.07888 \n",
      "Step 3760 loss 0.07819 \n",
      "Step 3770 loss 0.07723 \n",
      "Step 3780 loss 0.07605 \n",
      "Step 3790 loss 0.07564 \n",
      "Step 3800 loss 0.07506 \n",
      "Step 3810 loss 0.07436 \n",
      "Step 3820 loss 0.07371 \n",
      "Step 3830 loss 0.07311 \n",
      "Step 3840 loss 0.07241 \n",
      "Step 3850 loss 0.07184 \n",
      "Step 3860 loss 0.07073 \n",
      "Step 3870 loss 0.06995 \n",
      "Step 3880 loss 0.06932 \n",
      "Step 3890 loss 0.06828 \n",
      "Step 3900 loss 0.06742 \n",
      "Step 3910 loss 0.06665 \n",
      "Step 3920 loss 0.06539 \n",
      "Step 3930 loss 0.06476 \n",
      "Step 3940 loss 0.06438 \n",
      "Step 3950 loss 0.06329 \n",
      "Step 3960 loss 0.06231 \n",
      "Step 3970 loss 0.06126 \n",
      "Step 3980 loss 0.06061 \n",
      "Step 3990 loss 0.05942 \n",
      "Step 4000 loss 0.05856 \n",
      "Step 4010 loss 0.05753 \n",
      "Step 4020 loss 0.05661 \n",
      "Step 4030 loss 0.05622 \n",
      "Step 4040 loss 0.05581 \n",
      "Step 4050 loss 0.05513 \n",
      "Step 4060 loss 0.05449 \n",
      "Step 4070 loss 0.05403 \n",
      "Step 4080 loss 0.05368 \n",
      "Step 4090 loss 0.05328 \n",
      "Step 4100 loss 0.05274 \n",
      "Step 4110 loss 0.05199 \n",
      "Step 4120 loss 0.05128 \n",
      "Step 4130 loss 0.05080 \n",
      "Step 4140 loss 0.05058 \n",
      "Step 4150 loss 0.05009 \n",
      "Step 4160 loss 0.04916 \n",
      "Step 4170 loss 0.04865 \n",
      "Step 4180 loss 0.04816 \n",
      "Step 4190 loss 0.04752 \n",
      "Step 4200 loss 0.04692 \n",
      "Step 4210 loss 0.04642 \n",
      "Step 4220 loss 0.04603 \n",
      "Step 4230 loss 0.04565 \n",
      "Step 4240 loss 0.04502 \n",
      "Step 4250 loss 0.04419 \n",
      "Step 4260 loss 0.04373 \n",
      "Step 4270 loss 0.04324 \n",
      "Step 4280 loss 0.04281 \n",
      "Step 4290 loss 0.04237 \n",
      "Step 4300 loss 0.04185 \n",
      "Step 4310 loss 0.04160 \n",
      "Step 4320 loss 0.04114 \n",
      "Step 4330 loss 0.04067 \n",
      "Step 4340 loss 0.04004 \n",
      "Step 4350 loss 0.03902 \n",
      "Step 4360 loss 0.03839 \n",
      "Step 4370 loss 0.03775 \n",
      "Step 4380 loss 0.03731 \n",
      "Step 4390 loss 0.03637 \n",
      "Step 4400 loss 0.03592 \n",
      "Step 4410 loss 0.03565 \n",
      "Step 4420 loss 0.03529 \n",
      "Step 4430 loss 0.03485 \n",
      "Step 4440 loss 0.03455 \n",
      "Step 4450 loss 0.03427 \n",
      "Step 4460 loss 0.03395 \n",
      "Step 4470 loss 0.03339 \n",
      "Step 4480 loss 0.03323 \n",
      "Step 4490 loss 0.03276 \n",
      "Step 4500 loss 0.03228 \n"
     ]
    }
   ],
   "source": [
    "#train loop\n",
    "fit(x_f, t_f, x0, t0, u0, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights, tf_iter = 10000, newton_iter = 10000)\n",
    "\n",
    "\n",
    "#generate meshgrid for forward pass of u_pred\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact_u.T.flatten()[:,None]\n",
    "\n",
    "lb = np.array([-1.0, 0.0])\n",
    "ub = np.array([1.0, 1])\n",
    "\n",
    "u_pred, f_u_pred = predict(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "print('Error u: %e' % (error_u))\n",
    "\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/u1834031/Desktop/PID-GAN-main/PID-GAN/PDEs/AC/SA-PINN/SA-PINN.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/u1834031/Desktop/PID-GAN-main/PID-GAN/PDEs/AC/SA-PINN/SA-PINN.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/u1834031/Desktop/PID-GAN-main/PID-GAN/PDEs/AC/SA-PINN/SA-PINN.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m############################# Plotting ###############################\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/u1834031/Desktop/PID-GAN-main/PID-GAN/PDEs/AC/SA-PINN/SA-PINN.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/u1834031/Desktop/PID-GAN-main/PID-GAN/PDEs/AC/SA-PINN/SA-PINN.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((x0, \u001b[39m0\u001b[39m\u001b[39m*\u001b[39mx0), \u001b[39m1\u001b[39m) \u001b[39m# (x0, 0)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/u1834031/Desktop/PID-GAN-main/PID-GAN/PDEs/AC/SA-PINN/SA-PINN.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X_lb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((\u001b[39m0\u001b[39m\u001b[39m*\u001b[39mtb \u001b[39m+\u001b[39m lb[\u001b[39m0\u001b[39m], tb), \u001b[39m1\u001b[39m) \u001b[39m# (lb[0], tb)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/u1834031/Desktop/PID-GAN-main/PID-GAN/PDEs/AC/SA-PINN/SA-PINN.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m X_ub \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((\u001b[39m0\u001b[39m\u001b[39m*\u001b[39mtb \u001b[39m+\u001b[39m ub[\u001b[39m0\u001b[39m], tb), \u001b[39m1\u001b[39m) \u001b[39m# (ub[0], tb)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "############################# Plotting ###############################\n",
    "######################################################################\n",
    "\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "X_u_train = np.vstack([X0, X_lb, X_ub])\n",
    "\n",
    "# fig, ax = newfig(1.3, 1.0)\n",
    "ax.axis('off')\n",
    "\n",
    "####### Row 0: h(t,x) ##################\n",
    "gs0 = gridspec.GridSpec(1, 2)\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='YlGnBu',\n",
    "              extent=[lb[1], ub[1], lb[0], ub[0]],\n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[100]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[150]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "leg = ax.legend(frameon=False, loc = 'best')\n",
    "#    plt.setp(leg.get_texts(), color='w')\n",
    "ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "####### Row 1: h(t,x) slices ##################\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact_u[:,50], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.set_title('$t = %.2f$' % (t[50]), fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact_u[:,100], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[100,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.3), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact_u[:,150], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[150,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = %.2f$' % (t[150]), fontsize = 10)\n",
    "\n",
    "#show u_pred across domain\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "h = plt.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "            extent=[0.0, 1.0, -1.0, 1.0],\n",
    "            origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "plt.legend(frameon=False, loc = 'best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ec = plt.imshow(FU_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "            extent=[0.0, math.pi/2, -5.0, 5.0],\n",
    "            origin='lower', aspect='auto')\n",
    "\n",
    "#ax.add_collection(ec)\n",
    "ax.autoscale_view()\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$t$')\n",
    "cbar = plt.colorbar(ec)\n",
    "cbar.set_label('$\\overline{f}_u$ prediction')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t_f, x_f, c = col_weights.numpy(), s = col_weights.numpy()/10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
